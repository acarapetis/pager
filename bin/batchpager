#!/usr/bin/env python

# stdlib imports
import argparse
import glob
import os.path
import pathlib
import shutil
import sys
from datetime import datetime

# third party imports
import numpy as np
import pandas as pd
from impactutils.io.cmd import get_command_output

PAGER_COMMAND = "pagerall"


class CustomFormatter(
    argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter
):
    pass


def call_pagerall(cmd, grids, logfile, outfile):
    """Loop over a list of event IDs, calling pagerall on each one.

    Args:
        cmd (str): Path to pagerall command.
        grids (list): List of paths to grid.xml files to process.
        logfile (file): Open file object where child process can write output.
    Returns:
        tuple:
            (Boolean indicating success/failure,
            stdout from pagerall call,
            stderr from pagerall call)
    """
    for grid in grids:
        cmdstr = "%s -c %s >> %s" % (cmd, grid, outfile)
        res, stdout, stderr = get_command_output(cmdstr)
        if not res:
            logfile.write(
                'Failure for event %s: "%s" with command "%s"\n'
                % (grid, stdout + stderr, cmd)
            )
    return (res, stdout, stderr)


def main(args):
    # check for the existence of one of the pager programs
    pager_cmd = shutil.which(PAGER_COMMAND)
    if pager_cmd is None:
        print('Could not find "%s" on your path. Exiting.' % args.program)
        sys.exit(1)

    # now break up the input file into N chunks to process
    # read all the event ids
    events = open(args.file, "rt").readlines()
    # remove trailing newline from each event
    events = [event.strip() for event in events]

    chunks = np.array_split(events, args.num_processes)
    homedir = os.path.expanduser("~")
    logbase = "pager_batch_log_"
    outbase = "pager_batch_stdout_"
    logfmt = logbase + "%i.txt"
    outfmt = outbase + "%i.txt"

    # now run each chunk in parallel.
    child_ids = []
    for i in range(0, len(chunks)):
        try:
            pid = os.fork()
        except OSError:
            sys.stderr.write("Could not create a child process\n")
            continue

        if pid == 0:
            chunk = chunks[i]
            childid = os.getpid()
            logfile = os.path.join(homedir, logfmt % childid)
            flog = open(logfile, "wt")
            outfile = os.path.join(homedir, outfmt % childid)
            call_pagerall(pager_cmd, chunk, flog, outfile)
            flog.close()
            os._exit(0)
        else:
            print("Parent: created child process %i." % pid)
            child_ids.append(pid)

    for i in range(0, len(chunks)):
        child_id, _ = os.waitpid(0, 0)
        print("Child process %i has finished." % child_id)

    # read all the logfile content into a string, delete the logfiles...
    logfiles = glob.glob(os.path.join(homedir, logbase + "*"))
    errors = []
    for logfile in logfiles:
        content = open(logfile, "rt").read()
        os.remove(logfile)
        if not len(content):
            continue
        errors.append(content)
    errorstring = "\n".join(errors)
    if len(errorstring):
        print("Errors:\n")
        print(errorstring)

    # concatenate all of the output files into one file, ordered by the
    # child ids that we created.
    header_cols = [
        "EventID",
        "Time",
        "LocalTime",
        "Latitude",
        "Longitude",
        "Depth",
        "Magnitude",
        "Location",
        "EpicentralCountryCode",
        "CountryCode",
        "MMI01",
        "MMI02",
        "MMI03",
        "MMI04",
        "MMI05",
        "MMI06",
        "MMI07",
        "MMI08",
        "MMI09",
        "MMI10",
        "MaxMMI1000",
        "Fatalities",
        "EconMMI01",
        "EconMMI02",
        "EconMMI03",
        "EconMMI04",
        "EconMMI05",
        "EconMMI06",
        "EconMMI07",
        "EconMMI08",
        "EconMMI09",
        "EconMMI10",
        "Dollars",
    ]
    output = ",".join(header_cols) + "\n"
    for childid in child_ids:
        idfile = os.path.join(homedir, outfmt % childid)
        data = open(idfile, "rt").read()
        output += data
    todaystr = datetime.utcnow().strftime("%Y%m%d%H%M%S")
    outfile = os.path.join(homedir, "pagerall_%s.csv" % todaystr)

    # output = output.replace("\n\n", "\n")
    with open(outfile, "wt") as f:
        f.write(output)
    # print(outfile)
    dataframe = pd.read_csv(outfile, parse_dates=["Time"])
    outfile = pathlib.Path(outfile)
    outsheet = outfile.with_suffix(".xlsx")
    dataframe.to_excel(outsheet, index=False)
    outfile.unlink()
    nevents = len(dataframe["EventID"].unique())
    print(f"Wrote {len(dataframe)} rows ({nevents} events) to %s" % outsheet)


if __name__ == "__main__":
    description = """Population exposure in a batch process from a list of event IDs.

-n option should be chosen intelligently to be lower than the number of
cores present on the system.
    """
    parser = argparse.ArgumentParser(
        description=description, formatter_class=CustomFormatter
    )
    parser.add_argument("file", help="Text file with one grid file path per line.")

    nphlp = (
        "Number of instances of pagerall that should "
        "be run simultaneously. "
        "The value given to -n should generally be less than "
        "or equal to the number of cores on the machine."
    )
    parser.add_argument("-n", "--num-processes", help=nphlp, default=4, type=int)

    pargs = parser.parse_args()
    main(pargs)
